[
  {
    "objectID": "blog/2023/01_23_henrico_parcels/index.html",
    "href": "blog/2023/01_23_henrico_parcels/index.html",
    "title": "Age of Henrico County Virginia’s structures",
    "section": "",
    "text": "A recent tweet showed the age of the buildings in Groningen, Netherlands super-imposed on LiDAR data for some perspective.\nThis got me thinking – what would Henrico look like? I’m going to take the cheap way out and not mess with LiDAR at the moment, but the county provides their Tax Parcels and CAMA Data for easy download.\nThis is a pretty big file – just under 121 thousand parcel outlines, with 84 different bits of information per property!"
  },
  {
    "objectID": "blog/2023/01_23_henrico_parcels/index.html#enter-geoarrow",
    "href": "blog/2023/01_23_henrico_parcels/index.html#enter-geoarrow",
    "title": "Age of Henrico County Virginia’s structures",
    "section": "Enter: geoarrow",
    "text": "Enter: geoarrow\nI’ve been interested in playing around with the geoarrow package since watching Dewey Dunnington’s presentation at the 2022 RStudio conference. It leverages the Apache Arrow platform, resulting in rapid out-of-memory subsetting and manipulation of spatial data. The Parquet format used by Arrow also does a great job at compression, resulting in a much smaller file to toss around. When compared to a shapefile (which necessitates lugging around a bunch of other files), this seems like a much better way to store and access a large file."
  },
  {
    "objectID": "blog/2023/01_23_henrico_parcels/index.html#shp-to-.parquet",
    "href": "blog/2023/01_23_henrico_parcels/index.html#shp-to-.parquet",
    "title": "Age of Henrico County Virginia’s structures",
    "section": ".shp to .parquet",
    "text": ".shp to .parquet\nThe first step is to convert the provided shapefile into the arrow-readable parquet format. I’ll do this using the sf and geoarrow packages:\n\nlibrary(sf)\nlibrary(geoarrow)\n\nhenrico <- st_read('tax_parcels_and_cama_data.shp')\nwrite_geoparquet(henrico, 'data/henrico_parcels.parquet')\n\nEast enough – and switching from the shapefile to parquet reduced the file size by 70%!"
  },
  {
    "objectID": "blog/2023/01_23_henrico_parcels/index.html#pulling-in-the-data",
    "href": "blog/2023/01_23_henrico_parcels/index.html#pulling-in-the-data",
    "title": "Age of Henrico County Virginia’s structures",
    "section": "Pulling in the data",
    "text": "Pulling in the data\nUsing the newly-formed parquet file, it’s straightforward to only select two of the 84(!) columns and drop any structures without a construction date or a date that isn’t reasonable (in the future or before the settlement of the county).\n\nlibrary(arrow)\nlibrary(dplyr)\n\nhc_parcels <-\n  open_dataset('data/henrico_parcels.parquet') |> \n  select(YEAR_BUILT, geometry) |> \n  filter(!is.na(YEAR_BUILT) & YEAR_BUILT > 1500 & YEAR_BUILT < 2100) |> \n  geoarrow_collect_sf()"
  },
  {
    "objectID": "blog/2023/01_23_henrico_parcels/index.html#plot",
    "href": "blog/2023/01_23_henrico_parcels/index.html#plot",
    "title": "Age of Henrico County Virginia’s structures",
    "section": "Plot",
    "text": "Plot\nNow just a matter of plotting using ggplot2.\n\nlibrary(ggplot2)\n\nggplot(data = hc_parcels) +\n  geom_sf(aes(fill = YEAR_BUILT), lwd = 0, color = NA)+\n  scale_fill_viridis_c(option  = 'turbo') +\n  labs(fill = 'Year built') +\n  theme_dark() +\n  theme(legend.position = c(0.25, 0.25))\n\n\n\n\nThe structures built recently blend together in burnt red, while the older building really pop. What this really tells us, though, is that the majority of the development in Henrico County has happened since 1950 or so.\nIt makes sense that the number of structures built per year would be greater in, say, 2000 compared to 1700 – homes are easier to build and we have a much greater population. In order to see the subtle patterns in development in recent years, we need to adjust the color scale to change more rapidly as time goes on.\nThis can be accomplished using the values argument to scale_fill_viridis_c. Values need to be on the 0-to-1 scale. I’m going to shift the breaks in the color scale toward the last 25% of years.\n\nggplot(data = hc_parcels) +\n  geom_sf(aes(fill = YEAR_BUILT), lwd = 0, color = NA)+\n  scale_fill_viridis_c(option  = 'turbo',\n                       values = c(0, 0.5, 0.75, 0.85, 0.95, 1)) +\n  labs(fill = 'Year built') +\n  theme_dark() +\n  theme(legend.position = c(0.25, 0.25))"
  },
  {
    "objectID": "blog/2023/01_23_henrico_parcels/index.html#so-what-does-this-show-us",
    "href": "blog/2023/01_23_henrico_parcels/index.html#so-what-does-this-show-us",
    "title": "Age of Henrico County Virginia’s structures",
    "section": "So, what does this show us?",
    "text": "So, what does this show us?\n\nThe West End has a structure on nearly every parcel. This is especially apparent when comparing to Varina, where the blank gray spaces signify a plot of land with no listed structure.\nThe outskirts of Richmond (the “bite” in the middle) were developed heavily in the 1950s. The far West End, however, didn’t start to get developed until near 2000.\nThere’s a progressive structure to the development of the West End, as can be seen in the gradient of color."
  },
  {
    "objectID": "blog/2023/01_25_cbl_eel_essay/index.html",
    "href": "blog/2023/01_25_cbl_eel_essay/index.html",
    "title": "Elusive silver eel migrations detected in the Chesapeake mainstem for the first time",
    "section": "",
    "text": "Note: This short essay was originally posted in the December 2022 Chesapeake Biological Laboratory newsletter.\nWhen Sheila Eyler of the US Fish and Wildlife Service (USFWS) tagged 16 American eels 75 miles upstream of the Conowingo Dam she knew where they might end up – in the middle of the Atlantic Ocean in the Sargasso Sea – but whether they could traverse the major dams of the Susquehanna and how they would get there were still mysteries. Detections provided by the Chesapeake Biotelemetry Backbone, funded in part by the JES Avanti Foundation, have shown for the first time how these eels migrate out of the Bay.\nAmerican eels are born in the Sargasso Sea, hatching into transparent leptocephalus larvae while riding ocean currents into the mouth of the Chesapeake Bay. The larvae transform into glass eels and migrate upstream, where they live and grow as yellow eels for 5-20 years. When the eel is ready to spawn, they stop eating and turn into silver eels for their terminal trek back to the Sargasso Sea. Where, exactly, they go in the Sargasso is still unknown; when they spawn is only inferred from the presence of larvae.\nIdentifying when silver eels leave the Chesapeake may shed some light on this mysterious migration. For many years, we assumed the eels migrated between August and November – it’s when scattered silver eels appeared in pots and pound nets and it’s when they’re known to migrate in New England. However, it now looks like silver eels leave the Chesapeake during winter months. As is the case with other unexpected discoveries (see Atlantic sturgeon: Not the ‘ghosts’ I once thought they were, Bay Journal 2021), we were looking in the wrong place, at the wrong time, and with the wrong tools.\n\nEnter the Chesapeake Bay Biotelemetry Backbone: a broad partnership between the University of Maryland Center for Environmental Science, Maryland Department of Natural Resources, NOAA Chesapeake Bay Office, Smithsonian Environmental Research Center, and the Virginia Marine Resources Commission. The team aims to deploy biotelemetry arrays in a sustained, year-round manner that covers major portions of the Chesapeake Bay. Each array is located near “pinch-points” in the Chesapeake, chosen to maximize chance encounters with electronically tagged fish. Most importantly, the array is constantly listening for fish – even through the cold winter months – and does away with the need to directly capture fishes such as the cryptic silver eel.\nIn the first year of Eyler’s eel tagging, the Chesapeake Biological Laboratory’s backbone array detected five tagged Susquehanna eels passing by the mouth of the Patuxent River, MD. The silver eels blew by on their way to tropical Sargasso latitudes in the dead of winter – between late-December and mid-February – rather than fall months as in New England. Further, their stalwart migration emerging from the Conowingo and other major dams is good news for conservation efforts to restore American eels to the Susquehanna River. With USFWS planning to tag 200 more silver eels in the next 2 years and sustained deployment of the Chesapeake Bay Biotelemetry Backbone, knowledge of the timing and extent of Chesapeake silver eel migrations will only increase."
  },
  {
    "objectID": "blog/2023/02_21_wkb/index.html",
    "href": "blog/2023/02_21_wkb/index.html",
    "title": "Creating a simple features object via well-known-binary vs using coordinates",
    "section": "",
    "text": "If you work with fish telemetry data on the east coast of the USA, chances are that you’re now at least tangentially related to the Ocean Tracking Network (OTN).\nThe Ocean Tracking Network houses their data on a GeoServer, which often uses PostgreSQL/PostGIS behind the scenes. These databases store their spatial data in a format called “well-known binary” – as opposed to the human-readable “well-known text” you see in the output of an sf object.\nOTN data extracts export the WKB in a column called “the_geom”; it looks like a long string of numbers and letters. To investigate this, I’ll use the data set from Trudel 2018.\nThe main difference here is that the_geom can contain all of the information we may need, like coordinates, geometry type (points? polygons? multipoints? MULTIPOLYGONS???), and coordinate reference system. The latitude and longitude columns are just text: we need to infer/assume all of the other information.\nIn this particular case, that’s pretty easy. The latitude/longitude combinations are representing deployed receivers (points) and the are almost certainly in WGS84 (EPSG 4326) as that’s the system most-commonly used by a handheld GPS. We can provide this information directly and convert the CSV into an sf object.\nThe information we provided (coordinates and a coordinate reference system) helped fill out the metadata in the header and the well-known text (WKT) representation of the points in the “geometry” column. The analogous well-known binary (WKB) is contained within “the_geom” column. At this point, the WKB are just character strings.\nWe can convert the WKB as well, but it necessitates us jumping through some strange hoops. First we need to make “the_geom” have a “WKB” class.\nWe can then convert this to a simple features collection via st_as_sfc. Note that you may have to pass the EWKB = T argument if you come across some WKB in the wild, as PostGIS can create two types of WKB: Extended WKB and ISO WKB. EWKB allows other dimensions (like depth) and embedding a spatial reference identifier (SRID). ISO WKB also allows other identifiers, but no SRID. OTN seems to use ISO WKB as there is no CRS associated with the data.\nTo complete the cycle, we will convert the column to a simple features collection, then set it as the geometry of the original dataset.\nSo, is there any advantage to jumping through these hoops? Let’s benchmark it.\nSure doesn’t seem like it. After 100 iterations, parsing the binary is about four times slower than using st_as_sf. This result, combined with the fact that the code is more confusing and a PostGIS database is likely not being utilized by OTN’s end-users, suggests that the column may not get much use. Converting to EWKB may provide more use via adding a CRS, but the changes in the back end to make this happen probably make it so “the juice ain’t worth the squeeze.”"
  },
  {
    "objectID": "blog/2023/02_21_wkb/index.html#references",
    "href": "blog/2023/02_21_wkb/index.html#references",
    "title": "Creating a simple features object via well-known-binary vs using coordinates",
    "section": "References",
    "text": "References\nTrudel, Marc. “A Pilot Study to Investigate the Migration of Atlantic Salmon Post-Smolts and Their Interactions with Aquaculture in Passamaquoddy Bay, New Brunswick, Canada.” Ocean Tracking Network, 2018. https://members.oceantrack.org/project?ccode=PBSM.\nThis issue on GitHub: https://github.com/r-spatial/sf/issues/745#issuecomment-389778839"
  },
  {
    "objectID": "blog/2023/03_10_google_and_github_actions/index.html",
    "href": "blog/2023/03_10_google_and_github_actions/index.html",
    "title": "Connecting Google Sheets and GitHub Actions",
    "section": "",
    "text": "There are already a few blogs and presentations that outline the why and how of GitHub Actions and R:\nAnd also how to schedule R analysis to run on a schedule or connect with Google services:\nBecause of this, I’m going to skip right ahead to the part that always trips me up: connecting Google services like Google Drive and Google Sheets to an R script scheduled to run via GitHub Actions."
  },
  {
    "objectID": "blog/2023/03_10_google_and_github_actions/index.html#so-you-want-to-automate-saving-things-in-a-google-sheet",
    "href": "blog/2023/03_10_google_and_github_actions/index.html#so-you-want-to-automate-saving-things-in-a-google-sheet",
    "title": "Connecting Google Sheets and GitHub Actions",
    "section": "So, you want to automate saving things in a Google Sheet?",
    "text": "So, you want to automate saving things in a Google Sheet?\nWhen scheduling some code to run with GitHub Actions, a common workflow has been to run some code on schedule using GitHub Actions, save the results to Google Sheets, and allow my boss/peers to leverage the collaborative aspect of Google Sheets to comments and interact with the data product. Recently, this has taken the form of scraping data from websites that are frequently updated or display some sort of ephemeral data.\nThe most-difficult part of the whole process for me, however, has been getting the Google Sheets/Google Drive permissions set up. I’ve spent many hours failing at this, so this document is meant to leave a trail so we don’t have to fail twice."
  },
  {
    "objectID": "blog/2023/03_10_google_and_github_actions/index.html#permissions-needed",
    "href": "blog/2023/03_10_google_and_github_actions/index.html#permissions-needed",
    "title": "Connecting Google Sheets and GitHub Actions",
    "section": "Permissions needed",
    "text": "Permissions needed\nThe googledrive and googlesheets4 packages make it as easy as possible to interface with Google Drive and Google Sheets in R. Both leverage the gargle package to set up the security tokens needed to interface with all of the Google services.\nThe googledrive package uses the drive_auth function to interface with gargle and set up your authorization, while googlesheets4 uses gs4_auth. This process is easy, streamlined, and interactive – it opens up a browser window, you log into your Google account and tell it “yes, I want this to be able to access my files”, and that’s it! Unfortunately, if we’re running this in the cloud somewhere using GitHub Actions we’re not going to be there to click the button."
  },
  {
    "objectID": "blog/2023/03_10_google_and_github_actions/index.html#google-cloud-platform",
    "href": "blog/2023/03_10_google_and_github_actions/index.html#google-cloud-platform",
    "title": "Connecting Google Sheets and GitHub Actions",
    "section": "Google Cloud Platform",
    "text": "Google Cloud Platform\nBoth packages point to an article in gargle entitled “How to get your own API credentials”. This does an excellent job of walking through the how and why; the section of interest to us is “Service account token”.\nThe first step they outline is to go to the Google Cloud Platform website: https://console.cloud.google.com. Once you’re there, make sure you’re logged in to the correct account! Again, I emphasize:\nMAKE SURE YOU’RE LOGGED IN TO THE CORRECT ACCOUNT!\nLook in the top right, and make sure the correct profile picture is staring back at you. I ran into many hours of frustration trying everything to get permissions to work and experiencing failure after frustrating failure. Then I found out that I had made the key in my personal account rather than my institutional account.🤦\nOnce you are logged in to the CORRECT account, create a new project (or select an exiting one) using the drop-down menu next to the “GoogleCloud” banner at the top."
  },
  {
    "objectID": "blog/2023/03_10_google_and_github_actions/index.html#create-a-service-account-token",
    "href": "blog/2023/03_10_google_and_github_actions/index.html#create-a-service-account-token",
    "title": "Connecting Google Sheets and GitHub Actions",
    "section": "Create a service account token",
    "text": "Create a service account token\nNow we’ll create a service account token.\n\nClick the navigation menu, and select “IAM & Admin”, then “Service Accounts”.\nAt the top, next to “Service accounts” select “+ CREATE SERVICE ACCOUNT”.\nAdd a name (I suggest something somewhat descriptive) and the “Service account ID” section will autofill. Add a description if you desire.\nThis token will basically be a fake robot person – you can select what projects and permissions this fake person will have, or you can skip it. You can change these later – no biggy.\nClick “DONE”.\nClick your newly-created service account and go to the “KEYS” tab.\nClick “ADD KEY”, “Create new key”, select “JSON”, then “CREATE”.\nThis will download a file. Pay attention to where it goes!"
  },
  {
    "objectID": "blog/2023/03_10_google_and_github_actions/index.html#turn-on-the-google-sheetsdrive-api-for-your-gcp-project",
    "href": "blog/2023/03_10_google_and_github_actions/index.html#turn-on-the-google-sheetsdrive-api-for-your-gcp-project",
    "title": "Connecting Google Sheets and GitHub Actions",
    "section": "Turn on the Google Sheets/Drive API for your GCP project",
    "text": "Turn on the Google Sheets/Drive API for your GCP project\nNow, we’ll enable the Google Sheets and/or Google Drive API to allow the service account token to work.\n\nClick the menu, then “APIs & Services”.\nSearch for Google Sheets in the search bar.\nSelect “Google Sheets API” from the results.\nClick “ENABLE”.\nRepeat for Google Drive, if necessary."
  },
  {
    "objectID": "blog/2023/03_10_google_and_github_actions/index.html#fake-google-robot-people",
    "href": "blog/2023/03_10_google_and_github_actions/index.html#fake-google-robot-people",
    "title": "Connecting Google Sheets and GitHub Actions",
    "section": "Fake Google Robot People",
    "text": "Fake Google Robot People\nYou’ve basically just created a fake Google robot person. You need to give the fake-Google-robot-person access to the document (like you would a real person) using the fake-Google-robot-person email that is listed within the Service Accounts section.\n\nIn the “Service accounts” section we just visited, you can see your service account token listed by an email. It should be something like “YOUR-SERVICE-ACCOUNT-NAME@YOUR-GCP-PROJECT-NAME.iam.gserviceaccount.com”\nNavigate to your Google Drive/Sheet you want it to be able to access\nIf a Google Sheet, click “share” on the top right, paste in the robot-person email, and share it. For a Google Drive, navigate to the folder/item you’d like to share and do the same."
  },
  {
    "objectID": "blog/2023/03_10_google_and_github_actions/index.html#add-your-token-to-github-action-secrets",
    "href": "blog/2023/03_10_google_and_github_actions/index.html#add-your-token-to-github-action-secrets",
    "title": "Connecting Google Sheets and GitHub Actions",
    "section": "Add your token to GitHub Action secrets",
    "text": "Add your token to GitHub Action secrets\nWe’re nearing the end!\n\nGo into the GitHub repository that houses your GitHub Actions.\nClick “Settings”.\nUnder “Security” in the menu on the left side, select “Secrets and variables” and then “Actions”.\nClick “New repository secret”.\nCopy/paste the text in the file that was downloaded when you created your service account token and paste it into the “Secret” section.\nGive the secret a name.\nClick “Add secret”.\n\nYour service access token can now be added to the .Renviron of the runner created for your GitHub Action. Don’t think too hard about what this means – for us, it just means that we can access the token in a secret way using the name we just gave the secret using Sys.getenv('NAME_OF_MY_SECRET')."
  },
  {
    "objectID": "blog/2023/03_10_google_and_github_actions/index.html#coding-on-your-computer",
    "href": "blog/2023/03_10_google_and_github_actions/index.html#coding-on-your-computer",
    "title": "Connecting Google Sheets and GitHub Actions",
    "section": "Coding on your computer",
    "text": "Coding on your computer\nWhen working through the code on your computer, use the path argument to googledrive::drive_auth or googlesheets4::gs4_auth to tell R where you find your service access token.\n\n# for googledrive\ndrive_auth(path = 'path/to/my/service_token.json')\n\n# for googlesheets4\ngs4_auth(path = 'path/to/my/service_token.json')\n\nWhen you are ready to incorporate this into a GitHub Action, change this to Sys.getenv('NAME_OF_MY_SECRET').\n\ngs4_auth(path = Sys.getenv('NAME_OF_MY_SECRET'))"
  },
  {
    "objectID": "blog/2023/03_10_google_and_github_actions/index.html#github-actions",
    "href": "blog/2023/03_10_google_and_github_actions/index.html#github-actions",
    "title": "Connecting Google Sheets and GitHub Actions",
    "section": "GitHub Actions",
    "text": "GitHub Actions\nGoing over GitHub Actions and creating the needed workflow files is beyond the scope of this document.\nWorking back-to-front in the workflow YAML for web-scraping GitHub Action of mine, you can see that we have ask GitHub to pull the secret called “GDRIVE_PAT” (secret.GDRIVE_PAT) and assign it to a variable called GDRIVE_PAT (GDRIVE_PAT:) in our environment (env:).\njobs: \n  scrape_it:\n    runs-on: ubuntu-latest\n    env:\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n      GDRIVE_PAT: ${{ secrets.GDRIVE_PAT }}\nThis stores our access token in a variable called “GDRIVE_PAT” in the server’s .Renvrion. Now, when we write gs4_auth(path = Sys.getenv('GDRIVE_PAT')), our GitHub Action is allowed to access our Google Sheet! SUCCESS!\nThe neatest part is that this server is created and destroyed during this session, and our secret is never put out in the open. Completely secure!"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Connecting Google Sheets and GitHub Actions\n\n\n\n\n\n\n\nR\n\n\nWeb scraping\n\n\nGitHub Actions\n\n\nGoogle\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nMike O’Brien\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a simple features object via well-known-binary vs using coordinates\n\n\n\n\n\n\n\nR\n\n\nFish migration\n\n\nbiotelemetry\n\n\nsf\n\n\nspatial\n\n\nPostGIS\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\nMike O’Brien\n\n\n\n\n\n\n  \n\n\n\n\nElusive silver eel migrations detected in the Chesapeake mainstem for the first time\n\n\n\n\n\n\n\nChesapeake\n\n\nFish migration\n\n\nbiotelemetry\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2023\n\n\nMike O’Brien\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge of Henrico County Virginia’s structures\n\n\n\n\n\n\n\nR\n\n\nHenrico\n\n\nWeb scraping\n\n\nLand use\n\n\ngeoarrow\n\n\narrow\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2023\n\n\nMike O’Brien\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mike O'Brien",
    "section": "",
    "text": "University of Maryland, College Park | College Park, MD | Masters in Fisheries Science | August 2013\nUniversity of Miami | Coral Cables, FL | B.S. in Marine and Atmospheric Science | May 2009"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Mike O'Brien",
    "section": "Experience",
    "text": "Experience\nUniversity of Maryland Center for Environmental Science | Faculty Research Assistant | Sept 2013 - present"
  },
  {
    "objectID": "research/index.html#current",
    "href": "research/index.html#current",
    "title": "Research",
    "section": "Current",
    "text": "Current\nCheck out our current research offshore of Ocean City, MD:\nhttps://tailwinds.umces.edu/\nWhat’s going on? A haiku:\n\nTurbines going up\nBut how will the fish react?\nObserve: pots and hooks"
  },
  {
    "objectID": "research/index.html#journal-articles",
    "href": "research/index.html#journal-articles",
    "title": "Research",
    "section": "Journal articles",
    "text": "Journal articles\n Secor, D.H., Bailey, H., Carroll, A., Lyubchich, V., O’Brien, M.H.P., Wiernicki, C.J., 2021. Diurnal vertical movements in black sea bass (Centropristis striata): Endogenous, facultative, or something else? Ecosphere 12, e03616. https://doi.org/10.1002/ecs2.3616\n Secor, D.H., O’Brien, M.H.P., Coleman, N., Horne, A., Park, I., Kazyak, D.C., Bruce, D.G., Stence, C., 2021. Atlantic Sturgeon Status and Movement Ecology in an Extremely Small Spawning Habitat: The Nanticoke River-Marshyhope Creek, Chesapeake Bay. Reviews in Fisheries Science & Aquaculture 1–20. https://doi.org/10.1080/23308249.2021.1924617\n O’Brien, M.H.P., Secor, D.H., 2021. Influence of thermal stratification and storms on acoustic telemetry detection efficiency: a year-long test in the US Southern Mid-Atlantic Bight. Animal Biotelemetry 9, 8. https://doi.org/10.1186/s40317-021-00233-3\n Itakura, H., O’Brien, M.H.P., Secor, D., 2021. Tracking oxy-thermal habitat compression encountered by Chesapeake Bay striped bass through acoustic telemetry. ICES Journal of Marine Science fsab009. https://doi.org/10.1093/icesjms/fsab009\n Secor, D.H., O’Brien, M.H.P., Gahagan, B.I., Fox, D.A., Higgs, A.L., Best, J.E., 2020. Multiple spawning run contingents and population consequences in migratory striped bass Morone saxatilis. PLOS ONE 15(11): e0242797 https://doi.org/10.1371/journal.pone.0242797\n Rothermel, E.R., Balazik, M.T., Best, J.E., Fox, D.A., Gahagan, B.I., Haulsee, D.E., Higgs, A.L., O’Brien, M.H.P., Oliver, M.J., Park, I.A., and Secor, D.H. 2020. Comparative migration ecology of striped bass and Atlantic sturgeon in the US Southern Mid-Atlantic Bight flyway. PLOS ONE 15(6): e0234442. https://doi.org/10.1371/journal.pone.0234442\n Secor, D.H., O’Brien, M.H.P, Gahagan, B.I., Watterson, J.C., and Fox, D. 2020. Differential migration in Chesapeake Bay striped bass. PLOS ONE 15(5): e0233103. https://doi.org/10.1371/journal.pone.0233103\n Wiernicki, C.J., O’Brien, M.H.P., Zhang F, Lyubchich V, Li M, Secor, D.H., 2020. The recurring impact of storm disturbance on black sea bass (Centropristis striata) movement behaviors in the Mid-Atlantic Bight. PLOS ONE 15(12): e0239919. https://doi.org/10.1371/journal.pone.0239919\n Secor, D.H., Zhang, F., O’Brien, M.H.P. and Li, M. 2019. Ocean destratification and fish evacuation caused by a Mid-Atlantic tropical storm. ICES Journal of Marine Science, 76(2). https://doi.org/10.1093/icesjms/fsx241\n Wingfield, J.E., O’Brien, M., Lyubchich, V., Roberts, J.J., Halpin, P.N., Rice, A.N. and Bailey, H., 2017. Year-round spatiotemporal distribution of harbour porpoises within and around the Maryland wind energy area. PLOS ONE, 12(5), p.e0176653 https://doi.org/10.1371/journal.pone.0176653"
  },
  {
    "objectID": "research/index.html#open-reports",
    "href": "research/index.html#open-reports",
    "title": "Research",
    "section": "Open reports",
    "text": "Open reports\nSecor, D, M O’Brien, E Rothermel, C Wiernicki, and H Bailey. “Movement and Habitat Selection by Migratory Fishes within the Maryland Wind Energy Area and Adjacent Reference Sites.” Sterling (VA): U.S. Department of the Interior, Bureau of Ocean Energy Management, Office of Renewable Energy Programs, 2020. URL: https://espis.boem.gov/final%20reports/BOEM_2020-030.pdf"
  },
  {
    "objectID": "research/index.html#curated-datasets",
    "href": "research/index.html#curated-datasets",
    "title": "Research",
    "section": "Curated datasets",
    "text": "Curated datasets\nRothermel, Ella, Michael O’Brien, and David Secor. “Data from: Comparative Migration Ecology of Striped Bass and Atlantic Sturgeon in the US Southern Mid-Atlantic Bight Flyway.” Dryad, 2020. https://doi.org/10.5061/DRYAD.6HDR7SQX3.\nO’Brien, Michael, David Secor, Benjamin Gahagan, Dewayne Fox, Amanda Higgs, and Jessica Best. “Data from: Multiple Spawning Run Behavior and Population Consequences in Migratory Striped Bass Morone Saxatilis.” Dryad, 2020. https://doi.org/10.5061/DRYAD.6HDR7SQXT."
  },
  {
    "objectID": "research/packages.html",
    "href": "research/packages.html",
    "title": "Packages",
    "section": "",
    "text": "matos: An R API package for the Mid-Atlantic Acoustic Telemetry Observing System - https://matos.obrien.page\nTelemetryR: An R API package for the Atlantic Cooperative Telemetry (ACT) Network - https://github.com/mhpob/TelemetryR"
  },
  {
    "objectID": "rugby/2023/east-demographics/index.html#geographic-unions-gus",
    "href": "rugby/2023/east-demographics/index.html#geographic-unions-gus",
    "title": "East Demographics",
    "section": "Geographic Unions (GUs)",
    "text": "Geographic Unions (GUs)"
  },
  {
    "objectID": "rugby/index.html",
    "href": "rugby/index.html",
    "title": "Rugby",
    "section": "",
    "text": "Former:\n\nUSA Rugby National Panel (2016-2020)\nWorld Rugby 7s (2016-2018)\nMajor League Rugby (2018-2021)\n\nCurrent:\n\nPresident of the Mid-Atlantic Rugby Referees\nMember of the Potomac Society of Rugby Football Referees\n\n\n\n\n\n\n\n\n\nEast Demographics\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  }
]